{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF702 Redes Neurais\n",
    "Este notebook contém um script base para o projeto da disciplina IF702 Redes Neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura e Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A leitura dos dados é feita utilizando a biblioteca `pandas`. O presente exemplo importa a base de dados `mammography`. Caso você esteja trabalhando com outro data set, modifique este trecho de código.\n",
    "Para importar o conjunto de dados do PAKDD, use a função `pd.read_table` ao invés da `pd.read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_table('data/TRN')\n",
    "data_set.drop_duplicates(inplace=True)  # Remove exemplos repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INDEX</th>\n",
       "      <th>UF_1</th>\n",
       "      <th>UF_2</th>\n",
       "      <th>UF_3</th>\n",
       "      <th>UF_4</th>\n",
       "      <th>UF_5</th>\n",
       "      <th>UF_6</th>\n",
       "      <th>UF_7</th>\n",
       "      <th>IDADE</th>\n",
       "      <th>SEXO_1</th>\n",
       "      <th>...</th>\n",
       "      <th>CEP4_7</th>\n",
       "      <th>CEP4_8</th>\n",
       "      <th>CEP4_9</th>\n",
       "      <th>CEP4_10</th>\n",
       "      <th>CEP4_11</th>\n",
       "      <th>CEP4_12</th>\n",
       "      <th>CEP4_13</th>\n",
       "      <th>CEP4_14</th>\n",
       "      <th>IND_BOM_1_1</th>\n",
       "      <th>IND_BOM_1_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135098</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.273504</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.281910</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225741</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.480403</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   INDEX  UF_1  UF_2  UF_3  UF_4  UF_5  UF_6  UF_7     IDADE  SEXO_1  \\\n",
       "0      0     1     1     1     0     0     0     0  0.135098       1   \n",
       "1      1     1     0     1     0     0     1     0  0.273504       1   \n",
       "2      2     1     0     1     0     0     1     0  0.281910       0   \n",
       "3      3     1     1     1     0     0     0     0  0.225741       0   \n",
       "4      4     1     1     0     0     0     1     0  0.480403       0   \n",
       "\n",
       "      ...       CEP4_7  CEP4_8  CEP4_9  CEP4_10  CEP4_11  CEP4_12  CEP4_13  \\\n",
       "0     ...            0       0       1        1        0        1        1   \n",
       "1     ...            0       1       0        1        1        0        0   \n",
       "2     ...            1       1       0        0        0        0        1   \n",
       "3     ...            1       1       0        1        1        0        1   \n",
       "4     ...            1       1       1        0        0        1        0   \n",
       "\n",
       "   CEP4_14  IND_BOM_1_1  IND_BOM_1_2  \n",
       "0        1            0            1  \n",
       "1        0            1            0  \n",
       "2        0            1            0  \n",
       "3        0            1            0  \n",
       "4        1            1            0  \n",
       "\n",
       "[5 rows x 246 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exibe as 5 primeiras linhas do data set\n",
    "data_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INDEX</th>\n",
       "      <th>UF_1</th>\n",
       "      <th>UF_2</th>\n",
       "      <th>UF_3</th>\n",
       "      <th>UF_4</th>\n",
       "      <th>UF_5</th>\n",
       "      <th>UF_6</th>\n",
       "      <th>UF_7</th>\n",
       "      <th>IDADE</th>\n",
       "      <th>SEXO_1</th>\n",
       "      <th>...</th>\n",
       "      <th>CEP4_7</th>\n",
       "      <th>CEP4_8</th>\n",
       "      <th>CEP4_9</th>\n",
       "      <th>CEP4_10</th>\n",
       "      <th>CEP4_11</th>\n",
       "      <th>CEP4_12</th>\n",
       "      <th>CEP4_13</th>\n",
       "      <th>CEP4_14</th>\n",
       "      <th>IND_BOM_1_1</th>\n",
       "      <th>IND_BOM_1_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>389196.00000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>3.891960e+05</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>194597.50000</td>\n",
       "      <td>0.889274</td>\n",
       "      <td>0.691952</td>\n",
       "      <td>0.476552</td>\n",
       "      <td>0.296195</td>\n",
       "      <td>0.241179</td>\n",
       "      <td>0.218011</td>\n",
       "      <td>0.186836</td>\n",
       "      <td>4.552049e-01</td>\n",
       "      <td>0.521514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423378</td>\n",
       "      <td>0.417540</td>\n",
       "      <td>0.425708</td>\n",
       "      <td>0.459820</td>\n",
       "      <td>0.440842</td>\n",
       "      <td>0.436896</td>\n",
       "      <td>0.433709</td>\n",
       "      <td>0.440339</td>\n",
       "      <td>0.655449</td>\n",
       "      <td>0.344551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>112351.35202</td>\n",
       "      <td>0.313793</td>\n",
       "      <td>0.461687</td>\n",
       "      <td>0.499451</td>\n",
       "      <td>0.456579</td>\n",
       "      <td>0.427799</td>\n",
       "      <td>0.412895</td>\n",
       "      <td>0.389781</td>\n",
       "      <td>2.537459e-01</td>\n",
       "      <td>0.499538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494095</td>\n",
       "      <td>0.493154</td>\n",
       "      <td>0.494451</td>\n",
       "      <td>0.498384</td>\n",
       "      <td>0.496489</td>\n",
       "      <td>0.496002</td>\n",
       "      <td>0.495587</td>\n",
       "      <td>0.496428</td>\n",
       "      <td>0.475222</td>\n",
       "      <td>0.475222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.506237e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>97298.75000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.507866e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>194597.50000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.375241e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>291896.25000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.578835e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>389195.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              INDEX           UF_1           UF_2           UF_3  \\\n",
       "count  389196.00000  389196.000000  389196.000000  389196.000000   \n",
       "mean   194597.50000       0.889274       0.691952       0.476552   \n",
       "std    112351.35202       0.313793       0.461687       0.499451   \n",
       "min         0.00000       0.000000       0.000000       0.000000   \n",
       "25%     97298.75000       1.000000       0.000000       0.000000   \n",
       "50%    194597.50000       1.000000       1.000000       0.000000   \n",
       "75%    291896.25000       1.000000       1.000000       1.000000   \n",
       "max    389195.00000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                UF_4           UF_5           UF_6           UF_7  \\\n",
       "count  389196.000000  389196.000000  389196.000000  389196.000000   \n",
       "mean        0.296195       0.241179       0.218011       0.186836   \n",
       "std         0.456579       0.427799       0.412895       0.389781   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              IDADE         SEXO_1      ...               CEP4_7  \\\n",
       "count  3.891960e+05  389196.000000      ...        389196.000000   \n",
       "mean   4.552049e-01       0.521514      ...             0.423378   \n",
       "std    2.537459e-01       0.499538      ...             0.494095   \n",
       "min    5.506237e-16       0.000000      ...             0.000000   \n",
       "25%    2.507866e-01       0.000000      ...             0.000000   \n",
       "50%    4.375241e-01       1.000000      ...             0.000000   \n",
       "75%    6.578835e-01       1.000000      ...             1.000000   \n",
       "max    1.000000e+00       1.000000      ...             1.000000   \n",
       "\n",
       "              CEP4_8         CEP4_9        CEP4_10        CEP4_11  \\\n",
       "count  389196.000000  389196.000000  389196.000000  389196.000000   \n",
       "mean        0.417540       0.425708       0.459820       0.440842   \n",
       "std         0.493154       0.494451       0.498384       0.496489   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       1.000000       1.000000       1.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "             CEP4_12        CEP4_13        CEP4_14    IND_BOM_1_1  \\\n",
       "count  389196.000000  389196.000000  389196.000000  389196.000000   \n",
       "mean        0.436896       0.433709       0.440339       0.655449   \n",
       "std         0.496002       0.495587       0.496428       0.475222   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       1.000000   \n",
       "75%         1.000000       1.000000       1.000000       1.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "         IND_BOM_1_2  \n",
       "count  389196.000000  \n",
       "mean        0.344551  \n",
       "std         0.475222  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 246 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estatísticas sobre as variáveis\n",
    "data_set.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos separar o data set em atributos dependentes (X = features) e independentes (y = classe). No caso do `mammography` a classe majoritária está codificada como -1 e a classe minoritária está codificada como 1. Para treinar nossa rede neural precisamos que os valores de classe sejam 0 e 1 (saída da última camada é uma sigmóide), assim modificamos a codificação da majoritária para 0.\n",
    "\n",
    "Perceba que esse pré-processamento varia de data set para data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_set.drop(['INDEX', 'IND_BOM_1_1', 'IND_BOM_1_2'], axis=1)\n",
    "y = data_set[['IND_BOM_1_1', 'IND_BOM_1_2']]\n",
    "\n",
    "# IND_BOM_1_1 e IND_BOM_1_2 sao os indicadores de \"bom pagador\", o que se refere a conceder o credito\n",
    "# ou nao. Ou seja, a resposta de uma vai ser o inverso da outra. \n",
    "\n",
    "# Vamos utilzizar apenas uma como target, pois ambas representam \"a mesma coisa\".\n",
    "\n",
    "y = y.drop(['IND_BOM_1_2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão dos Dados em Treino, Validação, e Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui dividimos o data set em treino, validação e teste de maneira estratificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: 67049, class 1: 127549\n"
     ]
    }
   ],
   "source": [
    "## Treino: 50%, Validação: 25%, Teste: 25%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, \n",
    "                                                    random_state=42, shuffle=True, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/3, \n",
    "                                                  random_state=42, shuffle=True, stratify=y_train)\n",
    "\n",
    "train_class0 = y_train[y_train == 0] \n",
    "train_class0 = train_class0.dropna()\n",
    "train_class1 = y_train[y_train == 1] \n",
    "train_class1 = train_class1.dropna()\n",
    "\n",
    "print(\"class 0: {}, class 1: {}\".format(train_class0.shape[0], train_class1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling dos Dados e Normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar o comportamento da rede com diferentes funções de sampling, as mesmas devem ser implementadas e aplicadas ao conjunto de treinamento antes da normalização dos dados (você também pode investigar qual o efeito de aplicar o sampling após a normalização)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IND_BOM_1_1\n",
      "0     67049\n",
      "1    127549\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allyson/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de treinamento\n",
      "Class 0:  127549\n",
      "Class 1:  127549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allyson/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "## TO DO -- Implementar as funções de sampling a serem utilizadas\n",
    "\n",
    "y_train_group = y_train.groupby('IND_BOM_1_1')\n",
    "print(y_train_group.size())\n",
    "\n",
    "X_train_columns = X_train.columns\n",
    "\n",
    "# oversampling nos dados de treinamento\n",
    "X_train, y_train = SMOTE().fit_sample(X_train, y_train)\n",
    "print(\"Base de treinamento\")\n",
    "print(\"Class 0: \", np.where(y_train == 0)[0].shape[0])\n",
    "print(\"Class 1: \", np.where(y_train == 1)[0].shape[0])\n",
    "\n",
    "y_train = pd.DataFrame(np.array(y_train), columns=['IND_BOM_1_1'])\n",
    "X_train = pd.DataFrame(np.array(X_train), columns=X_train_columns)\n",
    "\n",
    "# oversampling nos dados de validação\n",
    "X_val, y_val = SMOTE().fit_sample(X_val, y_val)\n",
    "print(\"Base de validação\")\n",
    "print(\"Class 0: \", np.where(y_val == 0)[0].shape[0])\n",
    "print(\"Class 1: \", np.where(y_val == 1)[0].shape[0])\n",
    "\n",
    "y_val = pd.DataFrame(np.array(y_val), columns=['IND_BOM_1_1'])\n",
    "X_val = pd.DataFrame(np.array(X_val), columns=X_train_columns)\n",
    "\n",
    "\n",
    "#X_train.describe()\n",
    "df_train = pd.DataFrame(X_train)\n",
    "print(X_train)\n",
    "#f_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_data_files = open(\"sampling_data.csv\", \"w\")\n",
    "sampling_data_files.write(X_train.to_csv(index=False))\n",
    "sampling_data_files.close()\n",
    "sampling_labels = open(\"sampling_labels.csv\", \"w\")\n",
    "sampling_labels.write(y_train.to_csv(index=False))\n",
    "sampling_labels.close()\n",
    "sampling_validation = open(\"sampling_validation.csv\", \"w\")\n",
    "sampling_validation.write(X_val.to_csv(index=False))\n",
    "sampling_validation.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling_data_files = open(\"sampling_data.csv\", \"r\")\n",
    "data_x_train = pd.read_csv('sampling_data.csv')\n",
    "data_y_train = pd.read_csv('sampling_labels.csv')\n",
    "data_y_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_x_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "data_x_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante lembrar de normalizar os dados. A classe `StandardScaler` centraliza as variáveis e transforma as features para terem variância unitária. Você pode testar outras opções como o `MinMaxScaler`.\n",
    "\n",
    "Todas as alternativas estão disponíveis em:\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_x_train = scaler.fit_transform(data_x_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição e Treino da Rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui definimos a arquitetura da nossa rede neural e treinamos ela.\n",
    "\n",
    "No presente exemplo a rede possui apenas uma camada escondida. O código é bem intuitivo e a adição de novas camadas pode ser feita através da função `add`.\n",
    "\n",
    "Para treinar a rede várias funções de otimização estão disponíveis. \n",
    "\n",
    "Confira os exemplos em: https://keras.io/optimizers/\n",
    "\n",
    "O treinamento da rede pode ser interrompido baseado na performance dela em um conjunto de validação através de callbacks.\n",
    "\n",
    "Confira a documentação da classe `EarlyStopping`: https://keras.io/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de features do nosso data set.\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Aqui criamos o esboço da rede.\n",
    "classifier = Sequential()\n",
    "\n",
    "# Agora adicionamos a primeira camada escondida contendo 16 neurônios e função de ativação\n",
    "# tangente hiperbólica. Por ser a primeira camada adicionada à rede, precisamos especificar\n",
    "# a dimensão de entrada (número de features do data set).\n",
    "classifier.add(Dense(16, activation='tanh', input_dim=input_dim))\n",
    "\n",
    "classifier.add(Dense(30, activation='tanh'))\n",
    "classifier.add(Dense(30, activation='tanh'))\n",
    "\n",
    "# Em seguida adicionamos a camada de saída. Como nosso problema é binário só precisamos de\n",
    "# 1 neurônio com função de ativação sigmoidal. A partir da segunda camada adicionada keras já\n",
    "# consegue inferir o número de neurônios de entrada (16) e nós não precisamos mais especificar.\n",
    "classifier.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Por fim compilamos o modelo especificando um otimizador, a função de custo, e opcionalmente\n",
    "# métricas para serem observadas durante treinamento.\n",
    "classifier.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para treinar a rede passamos o conjunto de treinamento e especificamos o tamanho do mini-batch,\n",
    "# o número máximo de épocas, e opcionalmente callbacks. No seguinte exemplo utilizamos early\n",
    "# stopping para interromper o treinamento caso a performance não melhore em um conjunto de validação.\n",
    "history = classifier.fit(X_train, y_train, batch_size=64, epochs=100000, \n",
    "                         callbacks=[EarlyStopping(patience=10)], validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas funções auxiliares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_losses(history):\n",
    "    \"\"\"Função para extrair o melhor loss de treino e validação.\n",
    "    \n",
    "    Argumento(s):\n",
    "    history -- Objeto retornado pela função fit do keras.\n",
    "    \n",
    "    Retorno:\n",
    "    Dicionário contendo o melhor loss de treino e de validação baseado \n",
    "    no menor loss de validação.\n",
    "    \"\"\"\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    idx_min_val_loss = np.argmin(val_loss)\n",
    "    return {'train_loss': train_loss[idx_min_val_loss], 'val_loss': val_loss[idx_min_val_loss]}\n",
    "\n",
    "def plot_training_error_curves(history):\n",
    "    \"\"\"Função para plotar as curvas de erro do treinamento da rede neural.\n",
    "    \n",
    "    Argumento(s):\n",
    "    history -- Objeto retornado pela função fit do keras.\n",
    "    \n",
    "    Retorno:\n",
    "    A função gera o gráfico do treino da rede e retorna None.\n",
    "    \"\"\"\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_loss, label='Train')\n",
    "    ax.plot(val_loss, label='Validation')\n",
    "    ax.set(title='Training and Validation Error Curves', xlabel='Epochs', ylabel='Loss (MSE)')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def compute_performance_metrics(y, y_pred_class, y_pred_scores=None):\n",
    "    accuracy = accuracy_score(y, y_pred_class)\n",
    "    recall = recall_score(y, y_pred_class)\n",
    "    precision = precision_score(y, y_pred_class)\n",
    "    f1 = f1_score(y, y_pred_class)\n",
    "    performance_metrics = (accuracy, recall, precision, f1)\n",
    "    if y_pred_scores is not None:\n",
    "        auroc = roc_auc_score(y, y_pred_scores)\n",
    "        aupr = average_precision_score(y, y_pred_scores)\n",
    "        performance_metrics = performance_metrics + (auroc, aupr)\n",
    "    return performance_metrics\n",
    "\n",
    "def print_metrics_summary(accuracy, recall, precision, f1, auroc=None, aupr=None):\n",
    "    print()\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Accuracy:\", value=accuracy))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Recall:\", value=recall))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Precision:\", value=precision))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"F1:\", value=f1))\n",
    "    if auroc is not None:\n",
    "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUROC:\", value=auroc))\n",
    "    if aupr is not None:\n",
    "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUPR:\", value=aupr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_error_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predições no Conjunto de Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora utilizamos a nossa rede para fazer predições no conjunto de teste e computar métricas de desempenho.\n",
    "\n",
    "Além das métricas utilizadas aqui, mais métricas de desempenho podem ser encontradas em: http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fazer predições no conjunto de teste\n",
    "y_pred_scores = classifier.predict(X_test)\n",
    "y_pred_class = classifier.predict_classes(X_test, verbose=0)\n",
    "\n",
    "## Matriz de confusão\n",
    "print('Matriz de confusão no conjunto de teste:')\n",
    "print(confusion_matrix(y_test, y_pred_class))\n",
    "\n",
    "## Resumo dos resultados\n",
    "losses = extract_final_losses(history)\n",
    "print()\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"Train Loss:\", value=losses['train_loss']))\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"Validation Loss:\", value=losses['val_loss']))\n",
    "print('\\nPerformance no conjunto de teste:')\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_test, y_pred_class, y_pred_scores)\n",
    "print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comparando MLPs com outros classificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora iremos comparar a performance da nossa rede neural contra outros classificadores e por fim analisar a performance conjunta deles quando treinados em um ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função auxiliar para criar um classificador compatível com a API do pacote scikit-learn. Modifique a arquitetura da sua rede aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sklearn_compatible_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, activation='tanh', input_dim=input_dim))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria o classificador, treina, e avalia a performance no conjunto de validação. Apenas quando a melhor combinação de hyperparâmetros for escolhida o classificador deve ser avaliado no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = KerasClassifier(build_fn=create_sklearn_compatible_model, \n",
    "                          batch_size=64, epochs=100,\n",
    "                          verbose=0)\n",
    "mlp_clf.fit(X_train, y_train)\n",
    "mlp_pred_class = mlp_clf.predict(X_val)\n",
    "mlp_pred_scores = mlp_clf.predict_proba(X_val)[:, 1]\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_val, mlp_pred_class, mlp_pred_scores)\n",
    "print('Performance no conjunto de validação:')\n",
    "print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = SVC(probability=True)  # Modifique aqui os hyperparâmetros\n",
    "svc_clf.fit(X_train, y_train)\n",
    "svc_pred_class = svc_clf.predict(X_val)\n",
    "svc_pred_scores = svc_clf.predict_proba(X_val)[:, 1]\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_val, svc_pred_class, svc_pred_scores)\n",
    "print('Performance no conjunto de validação:')\n",
    "print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = GradientBoostingClassifier()  # Modifique aqui os hyperparâmetros\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred_class = gb_clf.predict(X_val)\n",
    "gb_pred_scores = gb_clf.predict_proba(X_val)[:, 1]\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_val, gb_pred_class, gb_pred_scores)\n",
    "print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()  # Modifique aqui os hyperparâmetros\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_pred_class = rf_clf.predict(X_val)\n",
    "rf_pred_scores = rf_clf.predict_proba(X_val)[:, 1]\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_val, rf_pred_class, rf_pred_scores)\n",
    "print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_ens_clf = KerasClassifier(build_fn=create_sklearn_compatible_model,\n",
    "                              batch_size=64, epochs=50, verbose=0)\n",
    "svc_ens_clf = SVC(probability=True)\n",
    "gb_ens_clf = GradientBoostingClassifier()\n",
    "rf_ens_clf = RandomForestClassifier()\n",
    "ens_clf = VotingClassifier([('mlp', mlp_ens_clf), ('svm', svc_ens_clf), ('gb', gb_ens_clf), ('rf', rf_ens_clf)], \n",
    "                           voting='soft')\n",
    "\n",
    "ens_clf.fit(X_train, y_train)\n",
    "ens_pred_class = ens_clf.predict(X_val)\n",
    "ens_pred_scores = ens_clf.predict_proba(X_val)[:, 1]\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_val, ens_pred_class, ens_pred_scores)\n",
    "print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos analisar a performance final de todos os classificadores treinados e de nosso ensemble no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pred_class = mlp_clf.predict(X_test)\n",
    "mlp_pred_scores = mlp_clf.predict_proba(X_test)[:, 1]\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_test, mlp_pred_class, mlp_pred_scores)\n",
    "print('MLP')\n",
    "print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)\n",
    "\n",
    "svc_pred_class = svc_clf.predict(X_test)\n",
    "svc_pred_scores = svc_clf.predict_proba(X_test)[:, 1]\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_test, svc_pred_class, svc_pred_scores)\n",
    "print('\\n\\nSupport Vector Machine')\n",
    "print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)\n",
    "\n",
    "gb_pred_class = gb_clf.predict(X_test)\n",
    "gb_pred_scores = gb_clf.predict_proba(X_test)[:, 1]\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_test, gb_pred_class, gb_pred_scores)\n",
    "print('\\n\\nGradient Boosting')\n",
    "print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)\n",
    "\n",
    "rf_pred_class = rf_clf.predict(X_test)\n",
    "rf_pred_scores = rf_clf.predict_proba(X_test)[:, 1]\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_test, rf_pred_class, rf_pred_scores)\n",
    "print('\\n\\nRandom Forest')\n",
    "print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)\n",
    "\n",
    "ens_pred_class = ens_clf.predict(X_test)\n",
    "ens_pred_scores = ens_clf.predict_proba(X_test)[:, 1]\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_test, ens_pred_class, ens_pred_scores)\n",
    "print('\\n\\nEnsemble')\n",
    "print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseado no código acima você pode facilmente criar um ensemble de MLPs ou de outros classificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
